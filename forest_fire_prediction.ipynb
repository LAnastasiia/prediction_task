{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using https://towardsdatascience.com/random-forest-in-python-24d0893d51c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read CSV, reformat 'day' and 'month' columns into separate columns for each type use them more as features. Partition into training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('forestfires.csv')\n",
    "df.dropna()\n",
    "df = pd.get_dummies(df) # Restructure cathegorical data into columns.\n",
    "\n",
    "X = df.drop([\"X\", \"Y\", \"area\"], axis=1).values\n",
    "y = df[\"area\"].values\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into two data sets training and one for validating.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different models.  <br>\n",
    "For compare predicted area to the test targets, and estimate the error of a model's predictions\n",
    "- calculate mean squared error\n",
    "- calculate mean difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979.2983582599644\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print(np.square(np.subtract(y_test,y_pred)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054.95\n"
     ]
    }
   ],
   "source": [
    "# Support Vectors Regression Model\n",
    "\n",
    "svm = SVR(gamma='auto')  # kernel defalut: Radial basis function\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(round(np.square(np.subtract(y_test,y_pred)).mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966.7083997863244\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression Model\n",
    "# (? not sure if makes sense for quite multi-classed classification tasks)\n",
    "\n",
    "clf = DecisionTreeRegressor()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(np.square(np.subtract(y_test,y_pred)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1269.8640383658617\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor Model\n",
    "# https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(np.square(np.subtract(y_test,y_pred)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973.0718275038669\n"
     ]
    }
   ],
   "source": [
    "clf = Lasso(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(np.square(np.subtract(y_test,y_pred)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, on this data set, Linear Regression model had bad accuraccy (estimated with mean square error), but couln't be outperformed by other models. \n",
    "Lasso method is often shows slightly better results.\n",
    "<br>\n",
    "Also it's pretty hard to compare, as each model's accuracy is changing depending on part of the dataset that was chosen in partition step. Random Forest Model and Support Vectors Regression Model are usually better when on 'unconvenient' data split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
